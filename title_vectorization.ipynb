{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization of the title of the items from meta datasets\n",
    "The idea of our design is to vectorize the title (and maybe later descriptions after proper processing of the text in that field) to compare the query vector with these vectors and find the closest items to user query. \n",
    "\n",
    "[Query and items vectorizer](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) that I used here is the sentence embedding model which is the fine-tuned version of microsoft Mpnet. After vectorization, I index the vectors with [Faiss](https://github.com/facebookresearch/faiss) which is a library for efficient similarity search and clustering of dense vectors.\n",
    "Finally I saved the vectors and indeces to `vectorized_texts_v2.pkl` and `faiss_index_v2.bin`, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from transformers import MPNetModel, MPNetTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/all_beauty_meta_amazon.csv', usecols=['parent_asin','title','description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Texts: 100%|██████████| 3519/3519 [02:37<00:00, 22.40it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ensure CUDA is available for PyTorch\n",
    "assert torch.cuda.is_available(), \"CUDA is not available. Please check your PyTorch installation and GPU settings.\"\n",
    "\n",
    "# Batch encoding function with GPU acceleration\n",
    "def encode_texts_in_batches_gpu(texts, batch_size=32):\n",
    "    tokenizer = MPNetTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "    model = MPNetModel.from_pretrained('sentence-transformers/all-mpnet-base-v2').cuda()  # Move model to GPU\n",
    "    model.eval()  # Evaluation mode\n",
    "\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding Texts\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        encoded_input = tokenizer(batch_texts, padding=True, truncation=True, max_length=128, return_tensors='pt').to('cuda')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "        embeddings = model_output.last_hidden_state.mean(dim=1).cpu().numpy()  # Move embeddings back to CPU\n",
    "        all_embeddings.extend(embeddings)\n",
    "    \n",
    "    return np.array(all_embeddings)\n",
    "\n",
    "# Vectorize texts\n",
    "# Assuming the correct column name is 'text'\n",
    "texts = df['title'].astype(str).tolist()\n",
    "\n",
    "# Proceed with encoding and other operations\n",
    "df['vector'] = list(encode_texts_in_batches_gpu(texts))\n",
    "\n",
    "# Continue with the rest of your operations\n",
    "\n",
    "\n",
    "# Saving DataFrame with vectors\n",
    "df.to_pickle(\"vectorized_texts_v2.pkl\")\n",
    "\n",
    "# Faiss index creation and saving\n",
    "d = df['vector'].iloc[0].shape[0]  # Dimension of vectors\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(np.vstack(df['vector'].values))\n",
    "faiss.write_index(index, \"faiss_index_v2.bin\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amazon23_slickdeal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
